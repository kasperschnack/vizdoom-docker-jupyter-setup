{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment(visible = False):\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"scenarios/health_gathering.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case defend_the_center scenario)\n",
    "    game.set_doom_scenario_path(\"scenarios/health_gathering.wad\")\n",
    "    game.set_window_visible(visible)\n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    # [[1,0,0],[0,1,0],[0,0,1]]\n",
    "    possible_actions  = np.identity(3,dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[80:,:]\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    resized_frame = transform.resize(normalized_frame, [FRAME_SIZE, FRAME_SIZE])\n",
    "    return resized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_deque():\n",
    "    return deque([np.zeros([FRAME_SIZE, FRAME_SIZE], dtype=np.int) for i in range(STACK_SIZE)], maxlen=STACK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'STACK_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7c44097d9b24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstacked_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_deque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e22a0c820a1b>\u001b[0m in \u001b[0;36minit_deque\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_deque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFRAME_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRAME_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTACK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTACK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'STACK_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "stacked_frames = init_deque()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(state, is_new_episode, stacked_frames = None):\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        stacked_frames = init_deque()\n",
    "        for _ in range(STACK_SIZE):\n",
    "            stacked_frames.append(frame)\n",
    "\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "    stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STACK_SIZE = 4\n",
    "FRAME_SIZE = 84\n",
    "\n",
    "\n",
    "### ENVIRONMENT HYPERPARAMETERS\n",
    "state_size = [84,84,4] # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size() # 3 possible actions: turn left, turn right, move forward\n",
    "stack_size = 4 # Defines how many frames are stacked together\n",
    "\n",
    "## TRAINING HYPERPARAMETERS\n",
    "learning_rate = 0.002\n",
    "num_epochs = 500 # Total epochs for training \n",
    "\n",
    "batch_size = 1000 # Each 1 is a timestep (NOT AN EPISODE) # YOU CAN CHANGE TO 5000 if you have GPU\n",
    "gamma = 0.95 # Discounting rate\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PGNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                tempArray = [None]\n",
    "                for i in state_size:\n",
    "                    tempArray.append(i)\n",
    "                \n",
    "                # We create the placeholders\n",
    "                # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "                # [None, 84, 84, 4]\n",
    "                self.inputs_= tf.placeholder(tf.float32, tempArray, name=\"inputs_\")\n",
    "                self.actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "                self.discounted_episode_rewards_ = tf.placeholder(tf.float32, [None, ], name=\"discounted_episode_rewards_\")\n",
    "            \n",
    "                \n",
    "                # Add this placeholder for having this variable in tensorboard\n",
    "                self.mean_reward_ = tf.placeholder(tf.float32, name=\"mean_reward\")\n",
    "                \n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                \"\"\"\n",
    "                First convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                # Input is 84x84x4\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                             filters = 32,\n",
    "                                             kernel_size = [8,8],\n",
    "                                             strides = [4,4],\n",
    "                                             padding = \"VALID\",\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv1\")\n",
    "\n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm1')\n",
    "\n",
    "                self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "                ## --> [20, 20, 32]\n",
    "            \n",
    "            with tf.name_scope(\"conv2\"):\n",
    "                \"\"\"\n",
    "                Second convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                     filters = 64,\n",
    "                                     kernel_size = [4,4],\n",
    "                                     strides = [2,2],\n",
    "                                     padding = \"VALID\",\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                     name = \"conv2\")\n",
    "\n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm2')\n",
    "\n",
    "                self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "                ## --> [9, 9, 64]\n",
    "            \n",
    "            with tf.name_scope(\"conv3\"):\n",
    "                \"\"\"\n",
    "                Third convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                     filters = 128,\n",
    "                                     kernel_size = [4,4],\n",
    "                                     strides = [2,2],\n",
    "                                     padding = \"VALID\",\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                     name = \"conv3\")\n",
    "\n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm3')\n",
    "\n",
    "                self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "                ## --> [3, 3, 128]\n",
    "            \n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "                ## --> [1152]\n",
    "            \n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                      units = 512,\n",
    "                                      activation = tf.nn.elu,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    name=\"fc1\")\n",
    "            \n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc, \n",
    "                                               kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              units = 3, \n",
    "                                            activation=None)\n",
    "            \n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "                \n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "                # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "                # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "                self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.actions)\n",
    "                self.loss = tf.reduce_mean(self.neg_log_prob * self.discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.train_opt = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-d50b3854544d>:38: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-10-d50b3854544d>:43: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-d50b3854544d>:95: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-d50b3854544d>:103: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the PGNetwork\n",
    "PGNetwork = PGNetwork(state_size, action_size, learning_rate)\n",
    "\n",
    "# Initialize Session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/test\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", PGNetwork.loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", PGNetwork.mean_reward_ )\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 84, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size, stacked_frames):\n",
    "    # Initialize lists: states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    \n",
    "    # Reward of batch is also a trick to keep track of how many timestep we made.\n",
    "    # We use to to verify at the end of each episode if > batch_size or not.\n",
    "    \n",
    "    # Keep track of how many episodes in our batch (useful when we'll need to calculate the average reward per episode)\n",
    "    episode_num  = 1\n",
    "    \n",
    "    # Launch a new episode\n",
    "    game.new_episode()\n",
    "        \n",
    "    # Get a new state\n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    while True:\n",
    "        # Run State Through Policy & Calculate Action\n",
    "        action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
    "                                                   feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "        \n",
    "        # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
    "        # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
    "        #30% chance that we take action a2)\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
    "                                  p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "        action = possible_actions[action]\n",
    "\n",
    "        # Perform action\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        # Store results\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            # The episode ends so no next state\n",
    "            next_state = np.zeros((84, 84), dtype=np.int)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            \n",
    "            # Append the rewards_of_batch to reward_of_episode\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            \n",
    "            # Calculate gamma Gt\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode))\n",
    "           \n",
    "            # If the number of rewards_of_batch > batch_size stop the minibatch creation\n",
    "            # (Because we have sufficient number of episode mb)\n",
    "            # Remember that we put this condition here, because we want entire episode (Monte Carlo)\n",
    "            # so we can't check that condition for each step but only if an episode is finished\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            # Reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            \n",
    "            # Add episode\n",
    "            episode_num += 1\n",
    "            \n",
    "            # Start a new episode\n",
    "            game.new_episode()\n",
    "\n",
    "            # First we need a state\n",
    "            state = game.get_state().screen_buffer\n",
    "\n",
    "            # Stack the frames\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "         \n",
    "        else:\n",
    "            # If not done, the next_state become the current state\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "                         \n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-793c29f25285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Gather training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mstates_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_of_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscounted_rewards_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m### These part is used for analytics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3706d7f290b6>\u001b[0m in \u001b[0;36mmake_batch\u001b[0;34m(batch_size, stacked_frames)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Get a new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-19da6d00cd9f>\u001b[0m in \u001b[0;36mstack_frames\u001b[0;34m(stacked_frames, state, is_new_episode)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_new_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Preprocess frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_new_episode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-97be371beae3>\u001b[0m in \u001b[0;36mpreprocess_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcropped_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mnormalized_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcropped_frame\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mresized_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mFRAME_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRAME_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresized_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.pyc\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    167\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m    168\u001b[0m                    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                    preserve_range=preserve_range)\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# n-dimensional interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.pyc\u001b[0m in \u001b[0;36mwarp\u001b[0;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    894\u001b[0m                                      mode=ndi_mode, order=order, cval=cval)\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m     \u001b[0m_clip_warp_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwarped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.pyc\u001b[0m in \u001b[0;36m_clip_warp_output\u001b[0;34m(input_image, output_image, order, mode, cval, clip)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \"\"\"\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mmin_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_amin\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "# Keep track of all rewards total for each batch\n",
    "allRewards = []\n",
    "\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    # Load the model\n",
    "    #saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    while epoch < num_epochs + 1:\n",
    "        # Gather training data\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\n",
    "\n",
    "        ### These part is used for analytics\n",
    "        # Calculate the total reward ot the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the mean reward of the batch\n",
    "        # Total rewards of batch / nb episodes in that batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the average reward of all training\n",
    "        # mean_reward_of_that_batch / epoch\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # Calculate maximum reward recorded \n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"-----------\")\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        # Feedforward, gradient and backpropagation\n",
    "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt], feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb \n",
    "                                                                    })\n",
    "\n",
    "        print(\"Training Loss: {}\".format(loss_))\n",
    "\n",
    "        # Write TF Summaries\n",
    "        summary = sess.run(write_op, feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb,\n",
    "                                                                    PGNetwork.mean_reward_: mean_reward_of_that_batch\n",
    "                                                                    })\n",
    "\n",
    "        #summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
    "        writer.add_summary(summary, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # Save Model\n",
    "        if epoch % 10 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
